{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5nTyYB570Ps",
        "colab_type": "text"
      },
      "source": [
        "# Project 3 - Semantic Code Search\n",
        "## Submitted by:\n",
        "### Dhaval Patel - DJP526, Akshat Khare - AK7674, Disha Papneja - DP3074"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER5QOBAU92a5",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8M-ThEJAHzz",
        "colab_type": "text"
      },
      "source": [
        "In this project we have used the CodeSearchNet Corpus and participated in the corresponding challenge. In this project we focused only on Python language and the associated dataset which contains about 0.5 million pairs of function-documentation pairs and about another 1.1 million functions without an associated documentation. We then submitted our Normalized Discounted Cumulative Gain (NDCG) score for only the human annotated examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXhHJ8ohAVyi",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we have implemented Bag of Words approach. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words. Further details about it can be found it this research paper: https://arxiv.org/pdf/1909.09436.pdf. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYPgl_SWpSI",
        "colab_type": "text"
      },
      "source": [
        "# Importing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R41jcoDG8Kge",
        "colab_type": "text"
      },
      "source": [
        "In this section we import the dataset and explore the format and structure of data. It is be useful to explore a small sample in order to understand the format and structure of the data. While the full dataset can be automatically downloaded with the /script/setup script located in this repo, we can alternatively download a subset of the data from S3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRSu9IR_06ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "pd.set_option('max_colwidth',300)\n",
        "from pprint import pprint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD4iJ6Zu8-3y",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and decompressing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK3QQiuX2V2m",
        "colab_type": "text"
      },
      "source": [
        "First we download the python dataset from https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZ49kjmGF4z",
        "colab_type": "code",
        "outputId": "15b0cd74-c6d4-4279-9bf4-982db463ba26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#Ref: https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb\n",
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-11 01:08:31--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.9.222\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.9.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940909997 (897M) [application/zip]\n",
            "Saving to: ‘python.zip’\n",
            "\n",
            "python.zip          100%[===================>] 897.32M  16.3MB/s    in 57s     \n",
            "\n",
            "2020-05-11 01:09:28 (15.9 MB/s) - ‘python.zip’ saved [940909997/940909997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBWW1jgh2dhs",
        "colab_type": "text"
      },
      "source": [
        "Now, we unzip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYBPzzDGIHR",
        "colab_type": "code",
        "outputId": "0b454ca2-b9f2-41fa-c58c-d7779a39c718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "# -o option overwrites the files without prompting    \n",
        "!unzip -o python.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  python.zip\n",
            "   creating: python/\n",
            "   creating: python/final/\n",
            "   creating: python/final/jsonl/\n",
            "   creating: python/final/jsonl/train/\n",
            "  inflating: python/final/jsonl/train/python_train_9.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_12.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_10.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_0.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_6.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_2.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_4.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_8.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_11.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_5.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_13.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_3.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_1.jsonl.gz  \n",
            "  inflating: python/final/jsonl/train/python_train_7.jsonl.gz  \n",
            "   creating: python/final/jsonl/test/\n",
            "  inflating: python/final/jsonl/test/python_test_0.jsonl.gz  \n",
            "   creating: python/final/jsonl/valid/\n",
            "  inflating: python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
            "  inflating: python_dedupe_definitions_v2.pkl  \n",
            "  inflating: python_licenses.pkl     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyPvOdDk9mXf",
        "colab_type": "text"
      },
      "source": [
        "The unzipped dataset also contains .gz files so now we decompress all the gzip files <br>\n",
        "The whole Python dataset is divided into 14 chunks and each part of the traning dataset contains 30000 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex9nXc72Jex0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decompress this gzip file\n",
        "!gzip -f -d python/final/jsonl/train/python_train_0.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_1.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_2.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_3.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_4.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_5.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_6.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_7.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_8.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_9.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_10.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_11.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_12.jsonl.gz\n",
        "!gzip -f -d python/final/jsonl/train/python_train_13.jsonl.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGkt3Dh89uMv",
        "colab_type": "text"
      },
      "source": [
        "Now, we can inspect any of the unzip files to see its contents:\n",
        "Read in the file and display the first row. The data is stored in JSON Lines format.\n",
        "We can utilize the fact that each line in the file is valid json, and display the first row in a more human readable form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c-acVbxJ2_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('python/final/jsonl/train/python_train_0.jsonl', 'r') as f:\n",
        "    sample_file_train = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqHNu2qwAAPr",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDdltP1oT62",
        "colab_type": "text"
      },
      "source": [
        "In this section we convert the testing and training datasets into dataframes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09xNK2h-tO1",
        "colab_type": "text"
      },
      "source": [
        "The function below selects minimum data row out of the whole dataset of a given file for showing purposes as showing a large datarow will be difficult to understand. So we show the smallest row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEBawou1J8L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to select the minimum data row out of the whole dataset\n",
        "def getMinimumDataRow(passedDataset):\n",
        "  minDataDisplay=0\n",
        "  for i in range(len(passedDataset)):\n",
        "    if(len(passedDataset[minDataDisplay])>len(passedDataset[i])):\n",
        "      minDataDisplay=i\n",
        "  return minDataDisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMACPCFsADpb",
        "colab_type": "text"
      },
      "source": [
        "We can see the json file with minimum datarow below, we have used pprint (pretty printer) here to show the json file in its proper format including indentations and proper spacing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWv-aEFoGVCH",
        "colab_type": "code",
        "outputId": "4f14f49c-d34a-48fc-84ee-646476944f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "source": [
        "indexToDisplay=getMinimumDataRow(sample_file_train)\n",
        "#print(json.loads(sample_file_train[indexToDisplay]))\n",
        "# for formated print --> use pprint\n",
        "pprint(json.loads(sample_file_train[indexToDisplay]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'code': 'async def add(ctx, left: int, right: int):\\n'\n",
            "         '    \"\"\"Adds two numbers together.\"\"\"\\n'\n",
            "         '    await ctx.send(left + right)',\n",
            " 'code_tokens': ['async',\n",
            "                 'def',\n",
            "                 'add',\n",
            "                 '(',\n",
            "                 'ctx',\n",
            "                 ',',\n",
            "                 'left',\n",
            "                 ':',\n",
            "                 'int',\n",
            "                 ',',\n",
            "                 'right',\n",
            "                 ':',\n",
            "                 'int',\n",
            "                 ')',\n",
            "                 ':',\n",
            "                 'await',\n",
            "                 'ctx',\n",
            "                 '.',\n",
            "                 'send',\n",
            "                 '(',\n",
            "                 'left',\n",
            "                 '+',\n",
            "                 'right',\n",
            "                 ')'],\n",
            " 'docstring': 'Adds two numbers together.',\n",
            " 'docstring_tokens': ['Adds', 'two', 'numbers', 'together', '.'],\n",
            " 'func_name': 'add',\n",
            " 'language': 'python',\n",
            " 'original_string': 'async def add(ctx, left: int, right: int):\\n'\n",
            "                    '    \"\"\"Adds two numbers together.\"\"\"\\n'\n",
            "                    '    await ctx.send(left + right)',\n",
            " 'partition': 'train',\n",
            " 'path': 'examples/basic_bot.py',\n",
            " 'repo': 'Rapptz/discord.py',\n",
            " 'sha': '05d4f7f9620ef33635d6ac965b26528e09cdaf5b',\n",
            " 'url': 'https://github.com/Rapptz/discord.py/blob/05d4f7f9620ef33635d6ac965b26528e09cdaf5b/examples/basic_bot.py#L19-L21'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITNxGgTqBHf1",
        "colab_type": "text"
      },
      "source": [
        "The below code combines all the training dataset files into one json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0-m2-iQMjzk",
        "colab_type": "code",
        "outputId": "aee7a8f2-9f04-4b2c-ecc3-fbc2405a5e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "howManyTrainDataSet=14\n",
        "train_data_list_json=[]\n",
        "\n",
        "for i in range(howManyTrainDataSet):\n",
        "  oneFile=[]\n",
        "  with open('python/final/jsonl/train/python_train_'+str(i)+'.jsonl', 'r') as f:\n",
        "    oneFile = f.readlines()\n",
        "  print(\"Total dataset in Train_\"+str(i)+\" is : \"+str(len(oneFile)))\n",
        "  train_data_list_json=train_data_list_json+oneFile\n",
        "\n",
        "print(\"Train Dataset has\",len(train_data_list_json),\"rows\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total dataset in Train_0 is : 30000\n",
            "Total dataset in Train_1 is : 30000\n",
            "Total dataset in Train_2 is : 30000\n",
            "Total dataset in Train_3 is : 30000\n",
            "Total dataset in Train_4 is : 30000\n",
            "Total dataset in Train_5 is : 30000\n",
            "Total dataset in Train_6 is : 30000\n",
            "Total dataset in Train_7 is : 30000\n",
            "Total dataset in Train_8 is : 30000\n",
            "Total dataset in Train_9 is : 30000\n",
            "Total dataset in Train_10 is : 30000\n",
            "Total dataset in Train_11 is : 30000\n",
            "Total dataset in Train_12 is : 30000\n",
            "Total dataset in Train_13 is : 22178\n",
            "Train Dataset has 412178 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "249nand64CRJ",
        "colab_type": "text"
      },
      "source": [
        "## Conversion from text to Dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72GmIZcIAXDo",
        "colab_type": "text"
      },
      "source": [
        "The function below converts the List dataset to dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1D4j1CPOdz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToDataFrame(data_list_json):\n",
        "  dfList=[]\n",
        "  oneRow=[]\n",
        "  columnNames=list(json.loads((data_list_json[0])).keys())\n",
        "  for i in range(len(data_list_json)):\n",
        "    oneList=[]\n",
        "    oneRow=data_list_json[i]\n",
        "    jsonString=json.loads(oneRow)\n",
        "    for oneKey in columnNames:\n",
        "      oneList.append(jsonString[oneKey])\n",
        "    dfList.append(oneList)\n",
        "  df=pd.DataFrame(dfList, columns =columnNames) \n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGnmxaW04L_s",
        "colab_type": "text"
      },
      "source": [
        "We now convert the training dataset into dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFXMHtXDPFSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=convertToDataFrame(train_data_list_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfsW1BvAcx3",
        "colab_type": "text"
      },
      "source": [
        "Now, after converting the dataset into dataframe, we display the first 5 rows of it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_rUks7vPqVt",
        "colab_type": "code",
        "outputId": "5ce8b56b-209e-49d3-e956-f9eaa43887ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>path</th>\n",
              "      <th>func_name</th>\n",
              "      <th>original_string</th>\n",
              "      <th>language</th>\n",
              "      <th>code</th>\n",
              "      <th>code_tokens</th>\n",
              "      <th>docstring</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>sha</th>\n",
              "      <th>url</th>\n",
              "      <th>partition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ageitgey/face_recognition</td>\n",
              "      <td>examples/face_recognition_knn.py</td>\n",
              "      <td>train</td>\n",
              "      <td>def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in s...</td>\n",
              "      <td>python</td>\n",
              "      <td>def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in s...</td>\n",
              "      <td>[def, train, (, train_dir, ,, model_save_path, =, None, ,, n_neighbors, =, None, ,, knn_algo, =, 'ball_tree', ,, verbose, =, False, ), :, X, =, [, ], y, =, [, ], # Loop through each person in the training set, for, class_dir, in, os, ., listdir, (, train_dir, ), :, if, not, os, ., path, ., isdir...</td>\n",
              "      <td>Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        &lt;train_dir&gt;/\\n        ├── &lt;person...</td>\n",
              "      <td>[Trains, a, k, -, nearest, neighbors, classifier, for, face, recognition, .]</td>\n",
              "      <td>c96b010c02f15e8eeb0f71308c641179ac1f19bb</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ageitgey/face_recognition</td>\n",
              "      <td>examples/face_recognition_knn.py</td>\n",
              "      <td>predict</td>\n",
              "      <td>def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_s...</td>\n",
              "      <td>python</td>\n",
              "      <td>def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_s...</td>\n",
              "      <td>[def, predict, (, X_img_path, ,, knn_clf, =, None, ,, model_path, =, None, ,, distance_threshold, =, 0.6, ), :, if, not, os, ., path, ., isfile, (, X_img_path, ), or, os, ., path, ., splitext, (, X_img_path, ), [, 1, ], [, 1, :, ], not, in, ALLOWED_EXTENSIONS, :, raise, Exception, (, \"Invalid im...</td>\n",
              "      <td>Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifie...</td>\n",
              "      <td>[Recognizes, faces, in, given, image, using, a, trained, KNN, classifier]</td>\n",
              "      <td>c96b010c02f15e8eeb0f71308c641179ac1f19bb</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ageitgey/face_recognition</td>\n",
              "      <td>examples/face_recognition_knn.py</td>\n",
              "      <td>show_prediction_labels_on_image</td>\n",
              "      <td>def show_prediction_labels_on_image(img_path, predictions):\\n    \"\"\"\\n    Shows the face recognition results visually.\\n\\n    :param img_path: path to image to be recognized\\n    :param predictions: results of the predict function\\n    :return:\\n    \"\"\"\\n    pil_image = Image.open(img_path).conv...</td>\n",
              "      <td>python</td>\n",
              "      <td>def show_prediction_labels_on_image(img_path, predictions):\\n    \"\"\"\\n    Shows the face recognition results visually.\\n\\n    :param img_path: path to image to be recognized\\n    :param predictions: results of the predict function\\n    :return:\\n    \"\"\"\\n    pil_image = Image.open(img_path).conv...</td>\n",
              "      <td>[def, show_prediction_labels_on_image, (, img_path, ,, predictions, ), :, pil_image, =, Image, ., open, (, img_path, ), ., convert, (, \"RGB\", ), draw, =, ImageDraw, ., Draw, (, pil_image, ), for, name, ,, (, top, ,, right, ,, bottom, ,, left, ), in, predictions, :, # Draw a box around the face u...</td>\n",
              "      <td>Shows the face recognition results visually.\\n\\n    :param img_path: path to image to be recognized\\n    :param predictions: results of the predict function\\n    :return:</td>\n",
              "      <td>[Shows, the, face, recognition, results, visually, .]</td>\n",
              "      <td>c96b010c02f15e8eeb0f71308c641179ac1f19bb</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L153-L181</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ageitgey/face_recognition</td>\n",
              "      <td>face_recognition/api.py</td>\n",
              "      <td>_rect_to_css</td>\n",
              "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\\n\\n    :param rect: a dlib 'rect' object\\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\\n    \"\"\"\\n    return rect.top(), rect.right(...</td>\n",
              "      <td>python</td>\n",
              "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\\n\\n    :param rect: a dlib 'rect' object\\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\\n    \"\"\"\\n    return rect.top(), rect.right(...</td>\n",
              "      <td>[def, _rect_to_css, (, rect, ), :, return, rect, ., top, (, ), ,, rect, ., right, (, ), ,, rect, ., bottom, (, ), ,, rect, ., left, (, )]</td>\n",
              "      <td>Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\\n\\n    :param rect: a dlib 'rect' object\\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order</td>\n",
              "      <td>[Convert, a, dlib, rect, object, to, a, plain, tuple, in, (, top, right, bottom, left, ), order]</td>\n",
              "      <td>c96b010c02f15e8eeb0f71308c641179ac1f19bb</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L32-L39</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ageitgey/face_recognition</td>\n",
              "      <td>face_recognition/api.py</td>\n",
              "      <td>_trim_css_to_bounds</td>\n",
              "      <td>def _trim_css_to_bounds(css, image_shape):\\n    \"\"\"\\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array...</td>\n",
              "      <td>python</td>\n",
              "      <td>def _trim_css_to_bounds(css, image_shape):\\n    \"\"\"\\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array...</td>\n",
              "      <td>[def, _trim_css_to_bounds, (, css, ,, image_shape, ), :, return, max, (, css, [, 0, ], ,, 0, ), ,, min, (, css, [, 1, ], ,, image_shape, [, 1, ], ), ,, min, (, css, [, 2, ], ,, image_shape, [, 0, ], ), ,, max, (, css, [, 3, ], ,, 0, )]</td>\n",
              "      <td>Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array\\n    :return: a trimmed plain tuple representation of th...</td>\n",
              "      <td>[Make, sure, a, tuple, in, (, top, right, bottom, left, ), order, is, within, the, bounds, of, the, image, .]</td>\n",
              "      <td>c96b010c02f15e8eeb0f71308c641179ac1f19bb</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        repo  ... partition\n",
              "0  ageitgey/face_recognition  ...     train\n",
              "1  ageitgey/face_recognition  ...     train\n",
              "2  ageitgey/face_recognition  ...     train\n",
              "3  ageitgey/face_recognition  ...     train\n",
              "4  ageitgey/face_recognition  ...     train\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY6QYXui4KHK",
        "colab_type": "text"
      },
      "source": [
        "Now, its turn to convert test dataset into dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEgsSgL2P4fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decompress this gzip file\n",
        "!gzip -f -d python/final/jsonl/test/python_test_0.jsonl.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJJPkqBQP7lw",
        "colab_type": "code",
        "outputId": "93e8eb2d-a805-406e-be07-07297bb348ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('python/final/jsonl/test/python_test_0.jsonl', 'r') as f:\n",
        "    test_list = f.readlines()\n",
        "\n",
        "print(\"Total Data rows in Test dataset\",len(test_list))\n",
        "df_test=convertToDataFrame(test_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Data rows in Test dataset 22176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Jj4a964XBQ",
        "colab_type": "text"
      },
      "source": [
        "Displaying the fist 5 rows of testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU-KEQrlbfY5",
        "colab_type": "code",
        "outputId": "cadcb509-703a-4d3e-aae9-973e58130cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>path</th>\n",
              "      <th>func_name</th>\n",
              "      <th>original_string</th>\n",
              "      <th>language</th>\n",
              "      <th>code</th>\n",
              "      <th>code_tokens</th>\n",
              "      <th>docstring</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>sha</th>\n",
              "      <th>url</th>\n",
              "      <th>partition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>soimort/you-get</td>\n",
              "      <td>src/you_get/extractors/youtube.py</td>\n",
              "      <td>YouTube.get_vid_from_url</td>\n",
              "      <td>def get_vid_from_url(url):\\n        \"\"\"Extracts video ID from URL.\\n        \"\"\"\\n        return match1(url, r'youtu\\.be/([^?/]+)') or \\\\n          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/watch/...</td>\n",
              "      <td>python</td>\n",
              "      <td>def get_vid_from_url(url):\\n        \"\"\"Extracts video ID from URL.\\n        \"\"\"\\n        return match1(url, r'youtu\\.be/([^?/]+)') or \\\\n          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\\n          match1(url, r'youtube\\.com/watch/...</td>\n",
              "      <td>[def, get_vid_from_url, (, url, ), :, return, match1, (, url, ,, r'youtu\\.be/([^?/]+)', ), or, match1, (, url, ,, r'youtube\\.com/embed/([^/?]+)', ), or, match1, (, url, ,, r'youtube\\.com/v/([^/?]+)', ), or, match1, (, url, ,, r'youtube\\.com/watch/([^/?]+)', ), or, parse_query_param, (, url, ,, '...</td>\n",
              "      <td>Extracts video ID from URL.</td>\n",
              "      <td>[Extracts, video, ID, from, URL, .]</td>\n",
              "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
              "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/youtube.py#L135-L143</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>soimort/you-get</td>\n",
              "      <td>src/you_get/extractors/miomio.py</td>\n",
              "      <td>sina_xml_to_url_list</td>\n",
              "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"str-&gt;list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.chil...</td>\n",
              "      <td>python</td>\n",
              "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"str-&gt;list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.chil...</td>\n",
              "      <td>[def, sina_xml_to_url_list, (, xml_data, ), :, rawurl, =, [, ], dom, =, parseString, (, xml_data, ), for, node, in, dom, ., getElementsByTagName, (, 'durl', ), :, url, =, node, ., getElementsByTagName, (, 'url', ), [, 0, ], rawurl, ., append, (, url, ., childNodes, [, 0, ], ., data, ), return, r...</td>\n",
              "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    From Biligrab.</td>\n",
              "      <td>[str, -, &gt;, list, Convert, XML, to, URL, List, ., From, Biligrab, .]</td>\n",
              "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
              "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>soimort/you-get</td>\n",
              "      <td>src/you_get/extractors/fc2video.py</td>\n",
              "      <td>makeMimi</td>\n",
              "      <td>def makeMimi(upid):\\n    \"\"\"From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110\"\"\"\\n    strSeed = \"gGddgPfeaf_gzyr\"\\n    prehash = upid + \"_\" + strSeed\\n    return md5(prehash.encode('utf-8')).hexdigest()</td>\n",
              "      <td>python</td>\n",
              "      <td>def makeMimi(upid):\\n    \"\"\"From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110\"\"\"\\n    strSeed = \"gGddgPfeaf_gzyr\"\\n    prehash = upid + \"_\" + strSeed\\n    return md5(prehash.encode('utf-8')).hexdigest()</td>\n",
              "      <td>[def, makeMimi, (, upid, ), :, strSeed, =, \"gGddgPfeaf_gzyr\", prehash, =, upid, +, \"_\", +, strSeed, return, md5, (, prehash, ., encode, (, 'utf-8', ), ), ., hexdigest, (, )]</td>\n",
              "      <td>From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110</td>\n",
              "      <td>[From, http, :, //, cdn37, ., atwikiimg, ., com, /, sitescript, /, pub, /, dksitescript, /, FC2, ., site, ., js, Also, com, ., hps, ., util, ., fc2, ., FC2EncrptUtil, ., makeMimiLocal, L110]</td>\n",
              "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
              "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L11-L17</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>soimort/you-get</td>\n",
              "      <td>src/you_get/extractors/fc2video.py</td>\n",
              "      <td>fc2video_download</td>\n",
              "      <td>def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\\n    \"\"\"wrapper\"\"\"\\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\\n    #'http:...</td>\n",
              "      <td>python</td>\n",
              "      <td>def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\\n    \"\"\"wrapper\"\"\"\\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\\n    #'http:...</td>\n",
              "      <td>[def, fc2video_download, (, url, ,, output_dir, =, '.', ,, merge, =, True, ,, info_only, =, False, ,, *, *, kwargs, ), :, #'http://video.fc2.com/en/content/20151021bTVKnbEw', #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw', #'http://video.fc2.com/ja/content/20151021bTVKnbEw', #'http://v...</td>\n",
              "      <td>wrapper</td>\n",
              "      <td>[wrapper]</td>\n",
              "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
              "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L46-L57</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>soimort/you-get</td>\n",
              "      <td>src/you_get/extractors/dailymotion.py</td>\n",
              "      <td>dailymotion_download</td>\n",
              "      <td>def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Dailymotion videos by URL.\\n    \"\"\"\\n\\n    html = get_content(rebuilt_url(url))\\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"(...</td>\n",
              "      <td>python</td>\n",
              "      <td>def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Dailymotion videos by URL.\\n    \"\"\"\\n\\n    html = get_content(rebuilt_url(url))\\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"(...</td>\n",
              "      <td>[def, dailymotion_download, (, url, ,, output_dir, =, '.', ,, merge, =, True, ,, info_only, =, False, ,, *, *, kwargs, ), :, html, =, get_content, (, rebuilt_url, (, url, ), ), info, =, json, ., loads, (, match1, (, html, ,, r'qualities\":({.+?}),\"', ), ), title, =, match1, (, html, ,, r'\"video_t...</td>\n",
              "      <td>Downloads Dailymotion videos by URL.</td>\n",
              "      <td>[Downloads, Dailymotion, videos, by, URL, .]</td>\n",
              "      <td>b746ac01c9f39de94cac2d56f665285b0523b974</td>\n",
              "      <td>https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              repo  ... partition\n",
              "0  soimort/you-get  ...      test\n",
              "1  soimort/you-get  ...      test\n",
              "2  soimort/you-get  ...      test\n",
              "3  soimort/you-get  ...      test\n",
              "4  soimort/you-get  ...      test\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmlHdfwP4b0j",
        "colab_type": "text"
      },
      "source": [
        "Finding the minium data row for display purposes like we did for the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGQhuwzkWqWz",
        "colab_type": "code",
        "outputId": "b23310a0-9db0-485a-ed2b-bba0b00728bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "# check minimum length data row from Test Dataset\n",
        "indexToDisplay=getMinimumDataRow(test_list)\n",
        "#print(json.loads(test_list[indexToDisplay]))\n",
        "# for formated print --> use pprint\n",
        "pprint(json.loads(test_list[indexToDisplay]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'code': 'def t_COMMA(self, t):\\n'\n",
            "         \"        r','\\n\"\n",
            "         '        t.endlexpos = t.lexpos + len(t.value)\\n'\n",
            "         '        return t',\n",
            " 'code_tokens': ['def',\n",
            "                 't_COMMA',\n",
            "                 '(',\n",
            "                 'self',\n",
            "                 ',',\n",
            "                 't',\n",
            "                 ')',\n",
            "                 ':',\n",
            "                 't',\n",
            "                 '.',\n",
            "                 'endlexpos',\n",
            "                 '=',\n",
            "                 't',\n",
            "                 '.',\n",
            "                 'lexpos',\n",
            "                 '+',\n",
            "                 'len',\n",
            "                 '(',\n",
            "                 't',\n",
            "                 '.',\n",
            "                 'value',\n",
            "                 ')',\n",
            "                 'return',\n",
            "                 't'],\n",
            " 'docstring': \"r',\",\n",
            " 'docstring_tokens': ['r'],\n",
            " 'func_name': 'ModelLoader.t_COMMA',\n",
            " 'language': 'python',\n",
            " 'original_string': 'def t_COMMA(self, t):\\n'\n",
            "                    \"        r','\\n\"\n",
            "                    '        t.endlexpos = t.lexpos + len(t.value)\\n'\n",
            "                    '        return t',\n",
            " 'partition': 'test',\n",
            " 'path': 'xtuml/load.py',\n",
            " 'repo': 'xtuml/pyxtuml',\n",
            " 'sha': '7dd9343b9a0191d1db1887ab9288d0a026608d9a',\n",
            " 'url': 'https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/xtuml/load.py#L449-L452'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwfpC5pHBbrI",
        "colab_type": "text"
      },
      "source": [
        "Displaying the type of function names present in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MNYtXXOc0gE",
        "colab_type": "code",
        "outputId": "ef5874d0-e98f-43ba-8522-d1c4a42b28cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Top 10 Type of function names present in the dataset\n",
        "df_test['func_name'].value_counts()[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "main                      105\n",
              "dump                       21\n",
              "get                        18\n",
              "parse                      17\n",
              "run                        16\n",
              "load                       14\n",
              "register                   14\n",
              "pull                       12\n",
              "Client._update_secrets     11\n",
              "connect                     9\n",
              "Name: func_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUBCa9dJBibh",
        "colab_type": "text"
      },
      "source": [
        "Displaying the names of columns in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWoVxtMA_6Mn",
        "colab_type": "code",
        "outputId": "4681f95f-5490-4bc7-e723-cf10ed4652be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['repo', 'path', 'func_name', 'original_string', 'language', 'code',\n",
              "       'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url',\n",
              "       'partition'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN9prhoZ42WH",
        "colab_type": "text"
      },
      "source": [
        "The function below converts tokens into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsbAB-IKZ3_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just to convert tokens to string\n",
        "def makeStr(listOfWords):\n",
        "  return ' '.join(listOfWords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7MiHwgx47d7",
        "colab_type": "text"
      },
      "source": [
        "Now, we convert the code_tokens column and docstring_tokens column to create a single string which will be used for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbJtLAvcKpI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use code_tokens and docstring_tokens to create a single string\n",
        "df_train['merged_tokens_str']=df_train['code_tokens'].apply(lambda oneList: makeStr(oneList))+\" \"+df_train['docstring_tokens'].apply(lambda oneList: makeStr(oneList))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOnvOnDrdqOU",
        "colab_type": "code",
        "outputId": "bac7d002-bb26-4d90-c126-51c5bca78585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "##############################\n",
        "# Training Parameter Setting #\n",
        "##############################\n",
        "#trainingDatasetSize=30\n",
        "trainingDatasetSize=df_train.shape[0]\n",
        "#############################\n",
        "#############################\n",
        "\n",
        "docs_train=list(df_train['merged_tokens_str'][:trainingDatasetSize])\n",
        "# Display few documents from training set\n",
        "print(docs_train[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['def train ( train_dir , model_save_path = None , n_neighbors = None , knn_algo = \\'ball_tree\\' , verbose = False ) : X = [ ] y = [ ] # Loop through each person in the training set for class_dir in os . listdir ( train_dir ) : if not os . path . isdir ( os . path . join ( train_dir , class_dir ) ) : continue # Loop through each training image for the current person for img_path in image_files_in_folder ( os . path . join ( train_dir , class_dir ) ) : image = face_recognition . load_image_file ( img_path ) face_bounding_boxes = face_recognition . face_locations ( image ) if len ( face_bounding_boxes ) != 1 : # If there are no people (or too many people) in a training image, skip the image. if verbose : print ( \"Image {} not suitable for training: {}\" . format ( img_path , \"Didn\\'t find a face\" if len ( face_bounding_boxes ) < 1 else \"Found more than one face\" ) ) else : # Add face encoding for current image to the training set X . append ( face_recognition . face_encodings ( image , known_face_locations = face_bounding_boxes ) [ 0 ] ) y . append ( class_dir ) # Determine how many neighbors to use for weighting in the KNN classifier if n_neighbors is None : n_neighbors = int ( round ( math . sqrt ( len ( X ) ) ) ) if verbose : print ( \"Chose n_neighbors automatically:\" , n_neighbors ) # Create and train the KNN classifier knn_clf = neighbors . KNeighborsClassifier ( n_neighbors = n_neighbors , algorithm = knn_algo , weights = \\'distance\\' ) knn_clf . fit ( X , y ) # Save the trained KNN classifier if model_save_path is not None : with open ( model_save_path , \\'wb\\' ) as f : pickle . dump ( knn_clf , f ) return knn_clf Trains a k - nearest neighbors classifier for face recognition .', 'def predict ( X_img_path , knn_clf = None , model_path = None , distance_threshold = 0.6 ) : if not os . path . isfile ( X_img_path ) or os . path . splitext ( X_img_path ) [ 1 ] [ 1 : ] not in ALLOWED_EXTENSIONS : raise Exception ( \"Invalid image path: {}\" . format ( X_img_path ) ) if knn_clf is None and model_path is None : raise Exception ( \"Must supply knn classifier either thourgh knn_clf or model_path\" ) # Load a trained KNN model (if one was passed in) if knn_clf is None : with open ( model_path , \\'rb\\' ) as f : knn_clf = pickle . load ( f ) # Load image file and find face locations X_img = face_recognition . load_image_file ( X_img_path ) X_face_locations = face_recognition . face_locations ( X_img ) # If no faces are found in the image, return an empty result. if len ( X_face_locations ) == 0 : return [ ] # Find encodings for faces in the test iamge faces_encodings = face_recognition . face_encodings ( X_img , known_face_locations = X_face_locations ) # Use the KNN model to find the best matches for the test face closest_distances = knn_clf . kneighbors ( faces_encodings , n_neighbors = 1 ) are_matches = [ closest_distances [ 0 ] [ i ] [ 0 ] <= distance_threshold for i in range ( len ( X_face_locations ) ) ] # Predict classes and remove classifications that aren\\'t within the threshold return [ ( pred , loc ) if rec else ( \"unknown\" , loc ) for pred , loc , rec in zip ( knn_clf . predict ( faces_encodings ) , X_face_locations , are_matches ) ] Recognizes faces in given image using a trained KNN classifier', 'def show_prediction_labels_on_image ( img_path , predictions ) : pil_image = Image . open ( img_path ) . convert ( \"RGB\" ) draw = ImageDraw . Draw ( pil_image ) for name , ( top , right , bottom , left ) in predictions : # Draw a box around the face using the Pillow module draw . rectangle ( ( ( left , top ) , ( right , bottom ) ) , outline = ( 0 , 0 , 255 ) ) # There\\'s a bug in Pillow where it blows up with non-UTF-8 text # when using the default bitmap font name = name . encode ( \"UTF-8\" ) # Draw a label with a name below the face text_width , text_height = draw . textsize ( name ) draw . rectangle ( ( ( left , bottom - text_height - 10 ) , ( right , bottom ) ) , fill = ( 0 , 0 , 255 ) , outline = ( 0 , 0 , 255 ) ) draw . text ( ( left + 6 , bottom - text_height - 5 ) , name , fill = ( 255 , 255 , 255 , 255 ) ) # Remove the drawing library from memory as per the Pillow docs del draw # Display the resulting image pil_image . show ( ) Shows the face recognition results visually .', 'def _rect_to_css ( rect ) : return rect . top ( ) , rect . right ( ) , rect . bottom ( ) , rect . left ( ) Convert a dlib rect object to a plain tuple in ( top right bottom left ) order', 'def _trim_css_to_bounds ( css , image_shape ) : return max ( css [ 0 ] , 0 ) , min ( css [ 1 ] , image_shape [ 1 ] ) , min ( css [ 2 ] , image_shape [ 0 ] ) , max ( css [ 3 ] , 0 ) Make sure a tuple in ( top right bottom left ) order is within the bounds of the image .', 'def face_distance ( face_encodings , face_to_compare ) : if len ( face_encodings ) == 0 : return np . empty ( ( 0 ) ) return np . linalg . norm ( face_encodings - face_to_compare , axis = 1 ) Given a list of face encodings compare them to a known face encoding and get a euclidean distance for each comparison face . The distance tells you how similar the faces are .', \"def load_image_file ( file , mode = 'RGB' ) : im = PIL . Image . open ( file ) if mode : im = im . convert ( mode ) return np . array ( im ) Loads an image file ( . jpg . png etc ) into a numpy array\", 'def _raw_face_locations ( img , number_of_times_to_upsample = 1 , model = \"hog\" ) : if model == \"cnn\" : return cnn_face_detector ( img , number_of_times_to_upsample ) else : return face_detector ( img , number_of_times_to_upsample ) Returns an array of bounding boxes of human faces in a image', 'def face_locations ( img , number_of_times_to_upsample = 1 , model = \"hog\" ) : if model == \"cnn\" : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , \"cnn\" ) ] else : return [ _trim_css_to_bounds ( _rect_to_css ( face ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , model ) ] Returns an array of bounding boxes of human faces in a image', 'def batch_face_locations ( images , number_of_times_to_upsample = 1 , batch_size = 128 ) : def convert_cnn_detections_to_css ( detections ) : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , images [ 0 ] . shape ) for face in detections ] raw_detections_batched = _raw_face_locations_batched ( images , number_of_times_to_upsample , batch_size ) return list ( map ( convert_cnn_detections_to_css , raw_detections_batched ) ) Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector If you are using a GPU this can give you much faster results since the GPU can process batches of images at once . If you aren t using a GPU you don t need this function .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-S-f2hWBx8I",
        "colab_type": "text"
      },
      "source": [
        "The code below preprocesses the data by removing the special characters, removing words of less length. Right now we are removing words having length less than 2 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXthpy4xJYl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "import string \n",
        "from nltk.stem import PorterStemmer\n",
        " \n",
        "# init stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "\n",
        "def textPreprocessor(text):\n",
        "\n",
        "  text=text.lower()\n",
        "  # Removes special characters\n",
        "  # Ref: https://kavita-ganesan.com/how-to-use-countvectorizer/#.XrXebWhKhPY\n",
        "  text=re.sub(\"\\\\W\",\" \",text)\n",
        "\n",
        "  # Ref: https://www.w3resource.com/python-exercises/re/python-re-exercise-49.php\n",
        "  # Removes words of less length --> right now it will remove the words which are of length between 1 and 2\n",
        "  shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "  text=shortword.sub('', text)\n",
        "\n",
        "  # stem words\n",
        "  words=re.split(\"\\\\s+\",text)\n",
        "  stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
        "  return ' '.join(stemmed_words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHffLUuoz4eR",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SioUKL1Q6ZBJ",
        "colab_type": "text"
      },
      "source": [
        "Now after loading and preprocessing the data, it's time to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsxDcfPrDikD",
        "colab_type": "text"
      },
      "source": [
        "## Using sklearns Tfidfvectorizer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95prpqqV6jWG",
        "colab_type": "text"
      },
      "source": [
        "TfidfVectorizer will tokenize documents using the textPreprocessor which is declared above this section and learns the vocabulary and calculates the idf (inverse document frequency) weights, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OngS5Qxz7C7h",
        "colab_type": "text"
      },
      "source": [
        "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document. Because these vectors will contain a lot of zeros, we call them sparse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0Nj4Zp7S1X",
        "colab_type": "text"
      },
      "source": [
        "The below code sets the parameters of TfidfVectorizer. The explanation of each parameter is as follows:\n",
        "* use_idf = whether to use idf (setting it to True) or just use tf only (setting it to False)\n",
        "* smooth_idf= used to Prevent zero divisions in tf-idf equation\n",
        "* ngram_range(min,max) = uses n-values as n-grams to be extracted from the documents\n",
        "* min_df=0.10 means, ignore words that have appeared in 10% or below 10% of the documents as they are too rare\n",
        "* max_df=0.85 means, ignore words appeared in 85% or above 85% of the documents as they are too common\n",
        "* preprocessor = cleaning text (stemming, removing special char etc.)\n",
        "* max_features = will keep the top max_features ordered by term frequency and drop the rest \n",
        "* binary =  just use presence or absence of a term instead of the raw counts. This is useful in some tasks such as certain features in text classification where the frequency of occurrence is insignificant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLFFvZwgml70",
        "colab_type": "text"
      },
      "source": [
        "#### Note: Above parameter settings can be run with various combinations if we have large computation power.\n",
        "#### For the experiment, we have kept the settings as low as possible to run it on Google Colab Memory limitations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vwkj_paOqys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ref: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "# Parameter Ref: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# Understanding ref: https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XrXeA2hKhPY\n",
        "\n",
        "# without Ngrams\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=(1,1),min_df=0.10,max_df=0.85,max_features=5000,preprocessor=textPreprocessor,binary=False)\n",
        "# With Ngrams of 1 and 2\n",
        "# tfidf_vectorizer = TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=(1,2),min_df=0.10,max_df=0.85,max_features=5000,preprocessor=textPreprocessor,binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROCHtr3c7K62",
        "colab_type": "text"
      },
      "source": [
        "Now after setting the parameters, we fit/get the vocabulary for the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KlO648ohJWC",
        "colab_type": "code",
        "outputId": "77c3e78c-68b4-4cf7-b781-f05b9b569889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# fit/make tfidf vector vocabulary with training dataset\n",
        "tfidf_vectorizer.fit(docs_train)\n",
        "# Dictonary of words and their indexes\n",
        "print(len(tfidf_vectorizer.vocabulary_))\n",
        "print(len(tfidf_vectorizer.stop_words_))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34\n",
            "1074432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXDB7zKI8UkF",
        "colab_type": "text"
      },
      "source": [
        "Now, we convert traning dataset into a vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyuZBJsvQZWm",
        "colab_type": "code",
        "outputId": "d1715027-b7a9-446f-c9f2-48ee56c15b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert each document to vector\n",
        "tfidf_vector_train=tfidf_vectorizer.transform(docs_train)\n",
        "print(tfidf_vector_train.shape)\n",
        "#print(tfidf_vector_train.toarray())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(412178, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t955x3Trawwa",
        "colab_type": "text"
      },
      "source": [
        "# Testing phase\n",
        "\n",
        " Now Convert the Testing set into vector\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIha7QKcoFB",
        "colab_type": "code",
        "outputId": "63a074e4-dddb-4803-a109-0d80f8d9b58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#############################\n",
        "# Testing Parameter Setting #\n",
        "#############################\n",
        "#testingDatasetSize=10\n",
        "#testingDatasetSize=df_test.shape[0]\n",
        "testingDatasetSize=25\n",
        "getTop=3\n",
        "#############################\n",
        "#############################\n",
        "\n",
        "df_test['merged_tokens_str']=df_test['code_tokens'].apply(lambda oneList: makeStr(oneList))+\" \"+df_test['docstring_tokens'].apply(lambda oneList: makeStr(oneList))\n",
        "\n",
        "docs_test=list(df_test['merged_tokens_str'][:testingDatasetSize])\n",
        "# Display few documents from testing set\n",
        "print(docs_test)\n",
        "\n",
        "tfidf_vector_test=tfidf_vectorizer.transform(docs_test)\n",
        "print(tfidf_vector_test.shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"def get_vid_from_url ( url ) : return match1 ( url , r'youtu\\\\.be/([^?/]+)' ) or match1 ( url , r'youtube\\\\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\\\\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\\\\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' ) Extracts video ID from URL .\", \"def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl str - > list Convert XML to URL List . From Biligrab .\", 'def makeMimi ( upid ) : strSeed = \"gGddgPfeaf_gzyr\" prehash = upid + \"_\" + strSeed return md5 ( prehash . encode ( \\'utf-8\\' ) ) . hexdigest ( ) From http : // cdn37 . atwikiimg . com / sitescript / pub / dksitescript / FC2 . site . js Also com . hps . util . fc2 . FC2EncrptUtil . makeMimiLocal L110', \"def fc2video_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : #'http://video.fc2.com/en/content/20151021bTVKnbEw' #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw' #'http://video.fc2.com/ja/content/20151021bTVKnbEw' #'http://video.fc2.com/tw/content/20151021bTVKnbEw' hostname = urlparse ( url ) . hostname if not ( 'fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname ) : return False upid = match1 ( url , r'.+/content/(\\\\w+)' ) fc2video_download_by_upid ( upid , output_dir , merge , info_only ) wrapper\", 'def dailymotion_download ( url , output_dir = \\'.\\' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r\\'qualities\":({.+?}),\"\\' ) ) title = match1 ( html , r\\'\"video_title\"\\\\s*:\\\\s*\"([^\"]+)\"\\' ) or match1 ( html , r\\'\"title\"\\\\s*:\\\\s*\"([^\"]+)\"\\' ) title = unicodize ( title ) for quality in [ \\'1080\\' , \\'720\\' , \\'480\\' , \\'380\\' , \\'240\\' , \\'144\\' , \\'auto\\' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge ) Downloads Dailymotion videos by URL .', 'def dictify ( r , root = True ) : if root : return { r . tag : dictify ( r , False ) } d = copy ( r . attrib ) if r . text : d [ \"_text\" ] = r . text for x in r . findall ( \"./*\" ) : if x . tag not in d : d [ x . tag ] = [ ] d [ x . tag ] . append ( dictify ( x , False ) ) return d http : // stackoverflow . com / a / 30923963 / 2946714', 'def ucas_download_single ( url , output_dir = \\'.\\' , merge = False , info_only = False , * * kwargs ) : html = get_content ( url ) # resourceID is UUID resourceID = re . findall ( r\\'resourceID\":\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\\' , html ) [ 0 ] assert resourceID != \\'\\' , \\'Cannot find resourceID!\\' title = match1 ( html , r\\'<div class=\"bc-h\">(.+)</div>\\' ) url_lists = _ucas_get_url_lists_by_resourceID ( resourceID ) assert url_lists , \\'Cannot find any URL of such class!\\' for k , part in enumerate ( url_lists ) : part_title = title + \\'_\\' + str ( k ) print_info ( site_info , part_title , \\'flv\\' , 0 ) if not info_only : download_urls ( part , part_title , \\'flv\\' , total_size = None , output_dir = output_dir , merge = merge ) video page', 'def ucas_download_playlist ( url , output_dir = \\'.\\' , merge = False , info_only = False , * * kwargs ) : html = get_content ( url ) parts = re . findall ( r\\'(getplaytitle.do\\\\?.+)\"\\' , html ) assert parts , \\'No part found!\\' for part_path in parts : ucas_download ( \\'http://v.ucas.ac.cn/course/\\' + part_path , output_dir = output_dir , merge = merge , info_only = info_only ) course page', \"def sina_download_by_vid ( vid , title = None , output_dir = '.' , merge = True , info_only = False ) : xml = api_req ( vid ) urls , name , size = video_info ( xml ) if urls is None : log . wtf ( name ) title = name print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( urls , title , 'flv' , size , output_dir = output_dir , merge = merge ) Downloads a Sina video by its unique vid . http : // video . sina . com . cn /\", \"def sina_download_by_vkey ( vkey , title = None , output_dir = '.' , merge = True , info_only = False ) : url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey type , ext , size = url_info ( url ) print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( [ url ] , title , 'flv' , size , output_dir = output_dir , merge = merge ) Downloads a Sina video by its unique vkey . http : // video . sina . com /\", 'def sina_download ( url , output_dir = \\'.\\' , merge = True , info_only = False , * * kwargs ) : if \\'news.sina.com.cn/zxt\\' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r\\'vid=(\\\\d+)\\' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r\\'hd_vid\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\' ) if hd_vid == \\'0\\' : vids = match1 ( video_page , r\\'[^\\\\w]vid\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\' ) . split ( \\'|\\' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r\\'vid:\"?(\\\\d+)\"?\\' ) if vid : #title = match1(video_page, r\\'title\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r\\'vkey\\\\s*:\\\\s*\"([^\"]+)\"\\' ) if vkey is None : vid = match1 ( url , r\\'#(\\\\d+)\\' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r\\'title\\\\s*:\\\\s*\"([^\"]+)\"\\' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only ) Downloads Sina videos by URL .', 'def yixia_download ( url , output_dir = \\'.\\' , merge = True , info_only = False , * * kwargs ) : hostname = urlparse ( url ) . hostname if \\'n.miaopai.com\\' == hostname : smid = match1 ( url , r\\'n\\\\.miaopai\\\\.com/media/([^.]+)\\' ) miaopai_download_by_smid ( smid , output_dir , merge , info_only ) return elif \\'miaopai.com\\' in hostname : #Miaopai yixia_download_by_scid = yixia_miaopai_download_by_scid site_info = \"Yixia Miaopai\" scid = match1 ( url , r\\'miaopai\\\\.com/show/channel/([^.]+)\\\\.htm\\' ) or match1 ( url , r\\'miaopai\\\\.com/show/([^.]+)\\\\.htm\\' ) or match1 ( url , r\\'m\\\\.miaopai\\\\.com/show/channel/([^.]+)\\\\.htm\\' ) or match1 ( url , r\\'m\\\\.miaopai\\\\.com/show/channel/([^.]+)\\' ) elif \\'xiaokaxiu.com\\' in hostname : #Xiaokaxiu yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid site_info = \"Yixia Xiaokaxiu\" if re . match ( r\\'http://v.xiaokaxiu.com/v/.+\\\\.html\\' , url ) : #PC scid = match1 ( url , r\\'http://v.xiaokaxiu.com/v/(.+)\\\\.html\\' ) elif re . match ( r\\'http://m.xiaokaxiu.com/m/.+\\\\.html\\' , url ) : #Mobile scid = match1 ( url , r\\'http://m.xiaokaxiu.com/m/(.+)\\\\.html\\' ) else : pass yixia_download_by_scid ( scid , output_dir , merge , info_only ) wrapper', \"def veoh_download ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : if re . match ( r'http://www.veoh.com/watch/\\\\w+' , url ) : item_id = match1 ( url , r'http://www.veoh.com/watch/(\\\\w+)' ) elif re . match ( r'http://www.veoh.com/m/watch.php\\\\?v=\\\\.*' , url ) : item_id = match1 ( url , r'http://www.veoh.com/m/watch.php\\\\?v=(\\\\w+)' ) else : raise NotImplementedError ( 'Cannot find item ID' ) veoh_download_by_id ( item_id , output_dir = '.' , merge = False , info_only = info_only , * * kwargs ) Get item_id\", 'def veoh_download_by_id ( item_id , output_dir = \\'.\\' , merge = False , info_only = False , * * kwargs ) : webpage_url = \\'http://www.veoh.com/m/watch.php?v={item_id}&quality=1\\' . format ( item_id = item_id ) #grab download URL a = get_content ( webpage_url , decoded = True ) url = match1 ( a , r\\'<source src=\"(.*?)\\\\\"\\\\W\\' ) #grab title title = match1 ( a , r\\'<meta property=\"og:title\" content=\"([^\"]*)\"\\' ) type_ , ext , size = url_info ( url ) print_info ( site_info , title , type_ , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = None , output_dir = output_dir , merge = merge ) Source : Android mobile', \"def download_by_id ( self , vid = '' , title = None , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : assert vid self . prepare ( vid = vid , title = title , * * kwargs ) self . extract ( * * kwargs ) self . download ( output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) self str - > None Keyword arguments : self : self vid : The video ID for BokeCC cloud something like FE3BB999594978049C33DC5901307461 Calls the prepare () to download the video . If no title is provided this method shall try to find a proper title with the information providin within the returned content of the API .\", 'def get_vid_from_url ( self , url ) : hit = re . search ( r\\'live.qq.com/(\\\\d+)\\' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r\\'live.qq.com/directory/match/(\\\\d+)\\' , url ) if hit is not None : return self . get_room_id_from_url ( hit . group ( 1 ) ) html = get_content ( url ) room_id = match1 ( html , r\\'room_id\\\\\":(\\\\d+)\\' ) if room_id is None : log . wtf ( \\'Unknown page {}\\' . format ( url ) ) return room_id Extracts video ID from live . qq . com .', 'def sprint ( text , * colors ) : return \"\\\\33[{}m{content}\\\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text Format text with color or other effects into ANSI escaped string .', 'def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script_name , text ) , * colors ) + \"\\\\n\" ) Print a log message to standard error .', 'def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code ) Print an error log message .', 'def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code ) What a Terrible Failure!', \"def detect_os ( ) : # Inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os Detect operating system .\", 'def miaopai_download_by_fid ( fid , output_dir = \\'.\\' , merge = False , info_only = False , * * kwargs ) : page_url = \\'http://video.weibo.com/show?fid=\\' + fid + \\'&type=mp4\\' mobile_page = get_content ( page_url , headers = fake_headers_mobile ) url = match1 ( mobile_page , r\\'<video id=.*?src=[\\\\\\'\"](.*?)[\\\\\\'\"]\\\\W\\' ) if url is None : wb_mp = re . search ( r\\'<script src=([\\\\\\'\"])(.+?wb_mp\\\\.js)\\\\1>\\' , mobile_page ) . group ( 2 ) return miaopai_download_by_wbmp ( wb_mp , fid , output_dir = output_dir , merge = merge , info_only = info_only , total_size = None , * * kwargs ) title = match1 ( mobile_page , r\\'<title>((.|\\\\n)+?)</title>\\' ) if not title : title = fid title = title . replace ( \\'\\\\n\\' , \\'_\\' ) ext , size = \\'mp4\\' , url_info ( url ) [ 2 ] print_info ( site_info , title , ext , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = None , output_dir = output_dir , merge = merge ) Source : Android mobile', \"def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\\\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs ) str - > None\", \"def vimeo_download_by_channel_id ( channel_id , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : html = get_content ( 'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}' . format ( channel_id = channel_id , access_token = access_token ) ) data = loads ( html ) id_list = [ ] #print(data) for i in data [ 'data' ] : id_list . append ( match1 ( i [ 'uri' ] , r'/videos/(\\\\w+)' ) ) for id in id_list : try : vimeo_download_by_id ( id , None , output_dir , merge , info_only , * * kwargs ) except urllib . error . URLError as e : log . w ( '{} failed with {}' . format ( id , e ) ) str / int - > None\", \"def vimeo_download_by_id ( id , title = None , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : site = VimeoExtractor ( ) site . download_by_vid ( id , info_only = info_only , output_dir = output_dir , merge = merge , * * kwargs ) try : # normal Vimeo video html = get_content ( https : // vimeo . com / + id ) cfg_patt = r clip_page_config \\\\ s * = \\\\ s * ( \\\\ { . + ? \\\\ } ) ; cfg = json . loads ( match1 ( html cfg_patt )) video_page = get_content ( cfg [ player ] [ config_url ] headers = fake_headers ) title = cfg [ clip ] [ title ] info = loads ( video_page ) except : # embedded player - referer may be required if referer in kwargs : fake_headers [ Referer ] = kwargs [ referer ]\"]\n",
            "(25, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_yJb8KkcTpD",
        "colab_type": "text"
      },
      "source": [
        "Now calcuate distance/relevance between for each testing document with each training document and give top documents from training set for each testing set. Meaning: We are giving top 3 recommendations(from training set) for each testing query\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFiYlol3qgwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding Distance between vector_traing and vector_test\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "topRelevence=[]\n",
        "\n",
        "for eachTestVector in tfidf_vector_test:\n",
        "  # computing cosine between one document from testing set with all documents fo training set\n",
        "  distanceBetweenOneTestRowAndAllTrainingRow=cosine_similarity(eachTestVector, tfidf_vector_train)\n",
        "  scores = distanceBetweenOneTestRowAndAllTrainingRow[0]\n",
        "  topScoreIndexes= list(np.argsort(scores))[-1*getTop:]\n",
        "  topScoreIndexes.reverse()\n",
        "  topRelevence.append(topScoreIndexes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__OTHWGE8kuz",
        "colab_type": "text"
      },
      "source": [
        "Displaying score for the first test document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb8AIfsnwS0",
        "colab_type": "code",
        "outputId": "9514e475-5c38-4292-f28b-d10c601e4cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Displaying score for the first test document\n",
        "print(topRelevence[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[84874, 85721, 20217]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj1ydxQU9Vzx",
        "colab_type": "text"
      },
      "source": [
        "In the below code we take the columns which are required in the CSV file which needs to be submitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nxvla89rneD",
        "colab_type": "code",
        "outputId": "ac3acdbf-953c-4339-986b-60346727b438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "# Now that we have top relevent documents from traning set for each testing document,\n",
        "# It's time to create the final submission file\n",
        "\n",
        "dfList=[]\n",
        "for oneIndex in range(testingDatasetSize):\n",
        "  for oneScore in range(getTop):\n",
        "    oneRow=[]\n",
        "    oneRow.append(df_test['docstring'][oneIndex])\n",
        "    oneRow.append(\"python\")\n",
        "    oneRow.append(df_test['func_name'][oneIndex])\n",
        "    oneRow.append(df_train['url'][topRelevence[oneIndex][oneScore]])\n",
        "    dfList.append(oneRow)\n",
        "  \n",
        "df=pd.DataFrame(dfList, columns =['query','language','identifier','url'])\n",
        "\n",
        "print(df.head())\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                         query  ...                                                                                                                                                 url\n",
            "0                                  Extracts video ID from URL.  ...                  https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/libs/comm_py/comm/ros/listenerBumper.py#L11-L36\n",
            "1                                  Extracts video ID from URL.  ...  https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/drivers/MAVLinkServer/MAVProxy/modules/lib/mp_util.py#L207-L214\n",
            "2                                  Extracts video ID from URL.  ...                     https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/utils/syntaxhighlighters.py#L852-L867\n",
            "3  str->list\\n    Convert XML to URL List.\\n    From Biligrab.  ...                                     https://github.com/adubkov/py-zabbix/blob/a26aadcba7c54cb122be8becc3802e2c42562c49/pyzabbix/sender.py#L282-L300\n",
            "4  str->list\\n    Convert XML to URL List.\\n    From Biligrab.  ...                          https://github.com/arteria/django-openinghours/blob/6bad47509a14d65a3a5a08777455f4cc8b4961fa/openinghours/forms.py#L23-L31\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9H0_1Jg9KI",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate with CodeSearchNet Challenge Benchmark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDiijNtJ9AYM",
        "colab_type": "text"
      },
      "source": [
        "Now that we have top relevent documents from traning set for each testing document, it's time to create the final submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y4bgCgu9Epu",
        "colab_type": "text"
      },
      "source": [
        "The benchmark evaluation dataset for the challenge can be found here: https://github.com/github/CodeSearchNet/blob/master/README.md#evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG6iEWt9m4z",
        "colab_type": "text"
      },
      "source": [
        "We have downloaded the benchmark query dataset (https://github.com/github/CodeSearchNet/blob/master/resources/queries.csv) into our github repository from which we are now loading the queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDy8SN3ghA1s",
        "colab_type": "code",
        "outputId": "599d31ec-1833-4271-db03-c184982b25ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!rm -rf AI-Project-3-CS-GY-6613\n",
        "!git clone https://github.com/dhavalpatel290/AI-Project-3-CS-GY-6613.git"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AI-Project-3-CS-GY-6613'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 28 (delta 4), reused 23 (delta 2), pack-reused 0\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVm4QnRBigae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store the results of predictions from Testing dataset\n",
        "df.to_csv(\"./AI-Project-3-CS-GY-6613/TF-IDF/results_On_Testing_Set_Of_2000_Rows/test_dataset_model_predictions.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nro_fqYH-2Qu",
        "colab_type": "text"
      },
      "source": [
        "Printing the first 5 lines of Benchmark Query dataset by CodeSearchNet Challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8yxfKwfi0cz",
        "colab_type": "code",
        "outputId": "49b1d508-6d8b-4995-9138-9a847c5a3f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df_test_leaderboard=pd.read_csv(\"./AI-Project-3-CS-GY-6613/TF-IDF/queries.csv\")\n",
        "print(df_test_leaderboard.head())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   query\n",
            "0  convert int to string\n",
            "1         priority queue\n",
            "2         string to date\n",
            "3       sort string list\n",
            "4      save list to file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ZYWclPjCTI",
        "colab_type": "code",
        "outputId": "9bdc325c-deac-42e1-8052-7664df4cdf4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "df_test_leaderboard['merged_tokens_str']=df_test_leaderboard['query']\n",
        "leaderboardTestingDatasetSize=df_test_leaderboard.shape[0]\n",
        "\n",
        "docs_test_leaderboard=list(df_test_leaderboard['merged_tokens_str'][:leaderboardTestingDatasetSize])\n",
        "# Display few queries from benchmark test set\n",
        "print(docs_test_leaderboard[:10])\n",
        "\n",
        "# get vector representation of each query\n",
        "tfidf_vector_test_leaderboard=tfidf_vectorizer.transform(docs_test_leaderboard)\n",
        "print(tfidf_vector_test_leaderboard.shape)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['convert int to string', 'priority queue', 'string to date', 'sort string list', 'save list to file', 'postgresql connection', 'confusion matrix', 'set working directory', 'group by count', 'binomial distribution']\n",
            "(99, 34)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_xtph9_kWQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get top 20 relevant documents for each test query\n",
        "topRelevenceLeaderboard=[]\n",
        "getTop=20\n",
        "for eachTestVector in tfidf_vector_test_leaderboard:\n",
        "  distanceBetweenOneTestRowAndAllTrainingRow=cosine_similarity(eachTestVector, tfidf_vector_train)\n",
        "  scores = distanceBetweenOneTestRowAndAllTrainingRow[0]\n",
        "  topScoreIndexes= list(np.argsort(scores))[-1*getTop:]\n",
        "  topScoreIndexes.reverse()\n",
        "  topRelevenceLeaderboard.append(topScoreIndexes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex2rGR8TkjiX",
        "colab_type": "code",
        "outputId": "65038179-de04-452c-e27e-dd8898e9136f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "dfListLeaderboard=[]\n",
        "for oneIndex in range(leaderboardTestingDatasetSize):\n",
        "  for oneScore in range(getTop):\n",
        "    oneRow=[]\n",
        "    oneRow.append(df_test_leaderboard['query'][oneIndex])\n",
        "    oneRow.append(\"python\")\n",
        "    oneRow.append(df_train['func_name'][topRelevenceLeaderboard[oneIndex][oneScore]])\n",
        "    oneRow.append(df_train['url'][topRelevenceLeaderboard[oneIndex][oneScore]])\n",
        "    dfListLeaderboard.append(oneRow)\n",
        "  \n",
        "df_leaderboard=pd.DataFrame(dfListLeaderboard,columns=['query','language','identifier','url'])\n",
        "\n",
        "# Display the computed results \n",
        "print(df_leaderboard.head())\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   query  ...                                                                                                                         url\n",
            "0  convert int to string  ...      https://github.com/pjuren/pyokit/blob/fddae123b5d817daa39496183f19c000d9c3791f/src/pyokit/datastruct/read.py#L264-L269\n",
            "1  convert int to string  ...  https://github.com/csparpa/pyowm/blob/cdd59eb72f32f7238624ceef9b2e2329a5ebd472/pyowm/alertapi30/alert_manager.py#L120-L148\n",
            "2  convert int to string  ...                https://github.com/csparpa/pyowm/blob/cdd59eb72f32f7238624ceef9b2e2329a5ebd472/pyowm/commons/tile.py#L71-L87\n",
            "3  convert int to string  ...               https://github.com/csparpa/pyowm/blob/cdd59eb72f32f7238624ceef9b2e2329a5ebd472/pyowm/commons/tile.py#L90-L107\n",
            "4  convert int to string  ...          https://github.com/csparpa/pyowm/blob/cdd59eb72f32f7238624ceef9b2e2329a5ebd472/pyowm/tiles/tile_manager.py#L34-L51\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buMTZ7KP_Jma",
        "colab_type": "text"
      },
      "source": [
        "Saving the CSV back to the repository, you can open our github repository to see the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr2b56oVk08Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_leaderboard.to_csv(\"AI-Project-3-CS-GY-6613/TF-IDF/results_On_Benchmark_Set_Of_100_Rows/model_predictions.csv\",index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDpijgKtyDQe",
        "colab_type": "code",
        "outputId": "fbeab1d2-f179-4992-cebe-9bfca2aa9a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/5a/cbdf36134804809d55ffd4c248343bd36680a92b6425885a3fd204d32f7b/pickle5-0.0.9.tar.gz (129kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.9-cp36-cp36m-linux_x86_64.whl size=218031 sha256=9b617e836c18e41a860900b3dfc665144ae8be8f2bf1628f89c0e1af76bcfa6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/1c/21/5e2fd8bfc197f237ab442a16d732d2591314229170eb98d290\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1nRPhTuogph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving Model\n",
        "# Ref: https://www.kaggle.com/mattwills8/fit-transform-and-save-tfidfvectorizer\n",
        "\n",
        "import pickle5 as pickle\n",
        "\n",
        "modelSaveFolder=\"AI-Project-3-CS-GY-6613/TF-IDF/savedModels/\"\n",
        "\n",
        "pickle.dump(tfidf_vectorizer, open(modelSaveFolder+\"tfidf.pickle\", \"wb\"))\n",
        "pickle.dump(tfidf_vector_train, open(modelSaveFolder+\"train_features.pickle\", \"wb\"))\n",
        "pickle.dump(tfidf_vector_test, open(modelSaveFolder+\"test_features.pickle\", \"wb\"))\n",
        "pickle.dump(tfidf_vector_test_leaderboard, open(modelSaveFolder+\"test_leaderboard_features.pickle\", \"wb\"))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hHiwRKE_c4H",
        "colab_type": "text"
      },
      "source": [
        "Now we download the result zip file to our local system which we will upload to our github repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXVg_2xlbOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -czf downloadFinalResult.tar.gz ./\"AI-Project-3-CS-GY-6613\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjfY4jKhZq_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('./downloadResults.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erYZVSHnNTEp",
        "colab_type": "text"
      },
      "source": [
        "## Submission on Wandb for the above run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRIceaxR0ZPD",
        "colab_type": "code",
        "outputId": "e89f0ee0-75b9-4b5d-cc9e-a141c19cb328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.6MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 17.1MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.2MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/configparser/\u001b[0m\n",
            "Collecting configparser>=3.8.1\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\u001b[0m\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.3MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=d2bf157abcd15792fc01d1707cbe36c1bea4277adf681615754e238cb7e02c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=94e0ca491ef2d527782205a3f05e964a5528b652f23aec1d4c4768150ae6c21b\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=c50eb4883866191801c1548a0d19d828e06eca316a0addeffb3adb639306039f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=7b6bf1222690a08abb232d7ded2bf1395f801e3169f8fae0714de036170551f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=25c4b2bdbafc0c6c81cb05f9d1486a4e51f0e5b0e99c85973898e7fb8022c4f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: shortuuid, docker-pycreds, pathtools, watchdog, smmap, gitdb, GitPython, subprocess32, graphql-core, gql, configparser, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.2 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.8.35 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gaa8mXWf0bUf",
        "colab_type": "code",
        "outputId": "373ed5a9-8d28-4a57-a573-fe62ce8642a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 068567082f04ce819f2f69fff51d3d04f2a20217\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TquV-t4X1cwy",
        "colab_type": "code",
        "outputId": "e7bf817f-9ec6-4753-d597-c4cb44bba115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import wandb\n",
        "wandb.init(project=\"AI-Project-3-CS-GY-6613-djp526\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/djp526/AI-Project-3-CS-GY-6613-djp526\" target=\"_blank\">https://app.wandb.ai/djp526/AI-Project-3-CS-GY-6613-djp526</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/djp526/AI-Project-3-CS-GY-6613-djp526/runs/2a7yfa69\" target=\"_blank\">https://app.wandb.ai/djp526/AI-Project-3-CS-GY-6613-djp526/runs/2a7yfa69</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/djp526/AI-Project-3-CS-GY-6613-djp526/runs/2a7yfa69"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3YuypLL1h_y",
        "colab_type": "code",
        "outputId": "04632ed8-3bfb-46ad-fecd-966e5d22435b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wandb.save(\"AI-Project-3-CS-GY-6613/TF-IDF/results_On_Benchmark_Set_Of_100_Rows/model_predictions.csv\")\n",
        "modelSaveFolder=\"AI-Project-3-CS-GY-6613/TF-IDF/savedModels/\"\n",
        "wandb.save(modelSaveFolder+\"tfidf.pickle\")\n",
        "wandb.save(modelSaveFolder+\"train_features.pickle\")\n",
        "wandb.save(modelSaveFolder+\"test_features.pickle\")\n",
        "wandb.save(modelSaveFolder+\"test_leaderboard_features.pickle\")\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20200511_013851-2a7yfa69/test_leaderboard_features.pickle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvDx4Yml-qJi",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heXcWkEcStyK",
        "colab_type": "text"
      },
      "source": [
        "Use of TF-IDF can lead to better results for the given benchmark queries if we could have used **n-gram models** with large size of vocabulary.\n",
        "\n",
        "To get better accuracy, we can leverage other NLP techniques of Document searching as well. In current research work in Google Search Engine they use **RNN** to get lingual representation of the search strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRp8hGvYVequ",
        "colab_type": "text"
      },
      "source": [
        "For Project's scope of implementation we have tried the model/training method which was given on CodeSearchNet github repository.\n",
        "\n",
        "We followed the instrcutions which were given here:  https://github.com/github/CodeSearchNet#setup\n",
        "\n",
        "Below are the settings for GPU: <br>\n",
        "Tesla M60 GPU <br> CPU count 16 <br> Memory 122 EBS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viy7DwMAY0eh",
        "colab_type": "text"
      },
      "source": [
        "The Code and Documentation for our CodeSearchNet Implementation is present here:\n",
        "https://github.com/dhavalpatel290/AI-Project-3-CS-GY-6613/tree/master/Baselinemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1h7zfYGZJO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}